# preprocess.py
import pandas as pd
import re
import nltk
import joblib
import logging
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import os

for resource in ["punkt", "punkt_tab", "stopwords"]:
    try:
        nltk.data.find(f"tokenizers/{resource}" if "punkt" in resource else f"corpora/{resource}")
        logging.info(f"‚úÖ NLTK resource '{resource}' found.")
    except LookupError:
        logging.info(f"üì• NLTK resource '{resource}' not found. Downloading...")
        nltk.download(resource)

# ---------------------------
# Setup logging
# ---------------------------
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler("preprocess.log", encoding="utf-8"),
        logging.StreamHandler()
    ]
)

logging.info("üöÄ Starting preprocessing...")

# ---------------------------
# Ensure NLTK resources are available
# ---------------------------
nltk_data_dir = os.path.join(os.path.expanduser("~"), "nltk_data")
if not os.path.exists(nltk_data_dir):
    os.makedirs(nltk_data_dir)

nltk.data.path.append(nltk_data_dir)

for resource in ["punkt", "stopwords"]:
    try:
        nltk.data.find(f"tokenizers/{resource}" if resource == "punkt" else f"corpora/{resource}")
        logging.info(f"‚úÖ NLTK resource '{resource}' found.")
    except LookupError:
        logging.info(f"üì• NLTK resource '{resource}' not found. Downloading...")
        nltk.download(resource, download_dir=nltk_data_dir)

# ---------------------------
# Load dataset
# ---------------------------
csv_file = "spotify_millsongdata.csv"  # make sure this matches your CSV name
try:
    df = pd.read_csv(csv_file).sample(10000)
    logging.info("‚úÖ Dataset loaded and sampled: %d rows", len(df))
except Exception as e:
    logging.error("‚ùå Failed to load dataset: %s", str(e))
    raise e

# Drop unnecessary columns
df = df.drop(columns=['link'], errors='ignore').reset_index(drop=True)

# ---------------------------
# Text cleaning
# ---------------------------
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    text = re.sub(r"[^a-zA-Z\s]", "", str(text))
    text = text.lower()
    tokens = word_tokenize(text)
    tokens = [word for word in tokens if word not in stop_words]
    return " ".join(tokens)

logging.info("üßπ Cleaning text...")
df['cleaned_text'] = df['text'].apply(preprocess_text)
logging.info("‚úÖ Text cleaned.")

# ---------------------------
# TF-IDF Vectorization
# ---------------------------
logging.info("üî† Vectorizing using TF-IDF...")
tfidf = TfidfVectorizer(max_features=5000)
tfidf_matrix = tfidf.fit_transform(df['cleaned_text'])
logging.info("‚úÖ TF-IDF matrix shape: %s", tfidf_matrix.shape)

# ---------------------------
# Cosine similarity
# ---------------------------
logging.info("üìê Calculating cosine similarity...")
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)
logging.info("‚úÖ Cosine similarity matrix generated.")

# ---------------------------
# Save preprocessed data
# ---------------------------
joblib.dump(df, 'df_cleaned.pkl')
joblib.dump(tfidf_matrix, 'tfidf_matrix.pkl')
joblib.dump(cosine_sim, 'cosine_sim.pkl')
logging.info("üíæ Preprocessed data saved to disk.")
logging.info("‚úÖ Preprocessing complete!")

